# -*- coding: utf-8 -*-
"""fsl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aFRxKkP9BTQiqldgAJihp-3Z8hn9OxVJ
"""

import os
import numpy as np
import keras
from keras import layers, models, ops
import cv2

# Configuration
DATASET_PATH = r"/content/drive/MyDrive/concave"
IMG_SIZE = 64
SUPPORT_SIZE = 5  # Examples per class in support set
QUERY_SIZE = 5    # Examples per class in query set
NUM_WAY = 5       # Number of classes per episode
EPISODES = 500
LEARNING_RATE = 0.001

# Load images
def load_images_from_directory(directory, img_size=IMG_SIZE):
    """Load images grouped by class"""
    data = {}

    for class_name in os.listdir(directory):
        class_path = os.path.join(directory, class_name)
        if not os.path.isdir(class_path):
            continue

        images = []
        for img_file in os.listdir(class_path):
            img_path = os.path.join(class_path, img_file)
            try:
                img = cv2.imread(img_path)
                if img is not None:
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (img_size, img_size))
                    img = img.astype('float32') / 255.0
                    images.append(img)
            except Exception as e:
                print(f"Error loading {img_path}: {e}")

        if images:
            data[class_name] = np.array(images)

    return data

# Embedding network
def build_embedding_network(input_shape=(IMG_SIZE, IMG_SIZE, 3)):
    """Encoder network that learns embeddings"""
    model = models.Sequential([
        layers.Input(shape=input_shape),

        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),

        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(64)  # Embedding dimension
    ])
    return model

class PrototypicalNetwork(models.Model):
    """Custom Prototypical Network Model"""

    def __init__(self, num_way, support_size, query_size, embedding_dim=64, **kwargs):
        super().__init__(**kwargs)
        self.num_way = num_way
        self.support_size = support_size
        self.query_size = query_size
        self.embedding_dim = embedding_dim

        self.embedding_net = build_embedding_network((IMG_SIZE, IMG_SIZE, 3))

    def call(self, inputs, training=None):
        support_images, query_images = inputs

        # support_images: (num_way * support_size, IMG_SIZE, IMG_SIZE, 3)
        # query_images: (num_way * query_size, IMG_SIZE, IMG_SIZE, 3)

        # Get embeddings
        support_embeddings = self.embedding_net(support_images, training=training)
        query_embeddings = self.embedding_net(query_images, training=training)

        # Compute prototypes (mean embedding per class)
        support_embeddings = ops.reshape(
            support_embeddings,
            (self.num_way, self.support_size, self.embedding_dim)
        )
        prototypes = ops.mean(support_embeddings, axis=1)  # (num_way, embedding_dim)

        # Compute distances between query and prototypes
        # query: (num_way * query_size, embedding_dim)
        # prototypes: (num_way, embedding_dim)

        query_reshaped = ops.reshape(
            query_embeddings,
            (self.num_way, self.query_size, self.embedding_dim)
        )

        # Compute euclidean distances
        distances = []
        for i in range(self.num_way):
            query_class = query_reshaped[i]  # (query_size, embedding_dim)
            # Compute distance to each prototype
            dists = ops.sqrt(
                ops.sum(ops.square(ops.expand_dims(query_class, 1) - prototypes), axis=2)
            )  # (query_size, num_way)
            distances.append(dists)

        distances = ops.concatenate(distances, axis=0)  # (num_way * query_size, num_way)

        # Convert distances to similarities (negative distances)
        logits = -distances

        return logits

def create_episode(data, num_way, support_size, query_size):
    """Create a training episode (task)"""
    selected_classes = np.random.choice(list(data.keys()), size=num_way, replace=False)

    support_images = []
    query_images = []
    query_labels = []

    for class_idx, class_name in enumerate(selected_classes):
        class_images = data[class_name]

        if len(class_images) < support_size + query_size:
            print(f"Warning: Class {class_name} has fewer images needed")
            continue

        indices = np.random.choice(
            len(class_images),
            size=support_size + query_size,
            replace=False
        )
        support_images.append(class_images[indices[:support_size]])
        query_images.extend(class_images[indices[support_size:support_size+query_size]])
        query_labels.extend([class_idx] * query_size)

    support_set = np.concatenate(support_images, axis=0)  # (num_way * support_size, ...)
    query_set = np.array(query_images)  # (num_way * query_size, ...)
    query_labels = np.array(query_labels)

    return support_set, query_set, query_labels

def main():
    print("Loading dataset...")
    data = load_images_from_directory(DATASET_PATH)

    if len(data) < NUM_WAY:
        print(f"Error: Need at least {NUM_WAY} classes, found {len(data)}")
        return

    print(f"Loaded {len(data)} classes")
    for class_name, images in data.items():
        print(f"  {class_name}: {len(images)} images")

    # Build model
    print("\nBuilding Prototypical Network...")
    model = PrototypicalNetwork(
        num_way=NUM_WAY,
        support_size=SUPPORT_SIZE,
        query_size=QUERY_SIZE
    )

    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy']
    )

    # Training loop
    print(f"Training for {EPISODES} episodes...")
    best_acc = 0

    for episode in range(EPISODES):
        support_set, query_set, query_labels = create_episode(
            data, NUM_WAY, SUPPORT_SIZE, QUERY_SIZE
        )

        # Create target labels (0-4 for 5 classes, repeated for each query)
        targets = np.repeat(np.arange(NUM_WAY), QUERY_SIZE)

        # Train step
        loss, acc = model.train_on_batch(
            [support_set, query_set],
            targets
        )

        if (episode + 1) % 10 == 0:
            print(f"Episode {episode + 1}/{EPISODES} - Loss: {loss:.4f}, Accuracy: {acc:.4f}")

        if acc > best_acc:
            best_acc = acc
            model.save('prototypical_network.keras')

    print(f"\nBest validation accuracy: {best_acc:.4f}")
    print("Model saved as 'prototypical_network.keras'")

    return model

if __name__ == "__main__":
    model = main()

